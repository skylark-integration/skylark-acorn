{"version":3,"sources":["main.js"],"names":["define","m_state","m_parseutil","m_statement","m_lval","m_expression","m_location","m_scope","m_options","m_locutil","m_node","m_tokentype","m_tokencontext","m_identifier","m_tokenize","m_whitespace","Parser","defaultOptions","Position","SourceLocation","getLineInfo","Node","TokenType","types","tokTypes","keywords","keywordTypes","TokContext","tokContexts","isIdentifierChar","isIdentifierStart","Token","isNewLine","lineBreak","lineBreakG","nonASCIIwhitespace","acorn","version","parse","input","options","parseExpressionAt","pos","tokenizer"],"mappings":";;;;;;;AAAAA,QACI,UACA,cACA,cACA,SACA,eACA,aACA,UACA,YACA,YACA,SACA,cACA,iBACA,eACA,aACA,gBACD,SAAUC,EAASC,EAAYC,EAAYC,EAAOC,EAAaC,EAAWC,EAAQC,EAAWC,EAAWC,EAAQC,EAAaC,EAAiBC,EAAcC,EAAYC,GACvK,aACA,MAAMC,OAACA,GAAUf,GACXgB,eAACA,GAAkBT,GACnBU,SAACA,EAAQC,eAAEA,EAAcC,YAAEA,GAAeX,GAC1CY,KAACA,GAAQX,GACTY,UAACA,EAAWC,MAAQC,EAAUC,SAAWC,GAAgBf,GACzDgB,WAACA,EAAWJ,MAAQK,GAAehB,GACnCiB,iBAACA,EAAgBC,kBAAEA,GAAqBjB,GACxCkB,MAACA,GAASjB,GACVkB,UAACA,EAASC,UAAEA,EAASC,WAAEA,EAAUC,mBAAEA,GAAsBpB,EAiC/D,OA9BAC,EAAOoB,OACHpB,OAAAA,EACAqB,QAHY,SAIZpB,eAAAA,EACAC,SAAAA,EACAC,eAAAA,EACAC,YAAAA,EACAC,KAAAA,EACAC,UAAAA,EACAE,SAAAA,EACAE,aAAAA,EACAC,WAAAA,EACAC,YAAAA,EACAC,iBAAAA,EACAC,kBAAAA,EACAC,MAAAA,EACAC,UAAAA,EACAC,UAAAA,EACAC,WAAAA,EACAC,mBAAAA,IAYAE,QAhCY,SAiCZrB,OAAAA,EACAC,eAAAA,EACAC,SAAAA,EACAC,eAAAA,EACAC,YAAAA,EACAC,KAAAA,EACAC,UAAAA,EACAE,SAAAA,EACAE,aAAAA,EACAC,WAAAA,EACAC,YAAAA,EACAC,iBAAAA,EACAC,kBAAAA,EACAC,MAAAA,EACAC,UAAAA,EACAC,UAAAA,EACAC,WAAAA,EACAC,mBAAAA,EACAG,MA7BJ,SAAeC,EAAOC,GAClB,OAAOxB,EAAOsB,MAAMC,EAAOC,IA6B3BC,kBA3BJ,SAA2BF,EAAOG,EAAKF,GACnC,OAAOxB,EAAOyB,kBAAkBF,EAAOG,EAAKF,IA2B5CG,UAzBJ,SAAmBJ,EAAOC,GACtB,OAAOxB,EAAO2B,UAAUJ,EAAOC","file":"../main.js","sourcesContent":["define([\n    './state',\n    './parseutil',\n    './statement',\n    './lval',\n    './expression',\n    './location',\n    './scope',\n    './options',\n    './locutil',\n    './node',\n    './tokentype',\n    './tokencontext',\n    './identifier',\n    './tokenize',\n    './whitespace'\n], function (m_state, m_parseutil,m_statement,m_lval,m_expression,m_location,m_scope,m_options, m_locutil, m_node, m_tokentype, m_tokencontext,  m_identifier, m_tokenize, m_whitespace) {\n    'use strict';\n    const {Parser} = m_state;\n    const {defaultOptions} = m_options;\n    const {Position, SourceLocation, getLineInfo} = m_locutil;\n    const {Node} = m_node;\n    const {TokenType, types : tokTypes, keywords : keywordTypes} = m_tokentype;\n    const {TokContext,types : tokContexts} = m_tokencontext;\n    const {isIdentifierChar, isIdentifierStart} = m_identifier;\n    const {Token} = m_tokenize;\n    const {isNewLine, lineBreak, lineBreakG, nonASCIIwhitespace} = m_whitespace;\n    \n    const version = '8.10.0';\n    Parser.acorn = {\n        Parser,\n        version,\n        defaultOptions,\n        Position,\n        SourceLocation,\n        getLineInfo,\n        Node,\n        TokenType,\n        tokTypes,\n        keywordTypes,\n        TokContext,\n        tokContexts,\n        isIdentifierChar,\n        isIdentifierStart,\n        Token,\n        isNewLine,\n        lineBreak,\n        lineBreakG,\n        nonASCIIwhitespace\n    };\n    function parse(input, options) {\n        return Parser.parse(input, options);\n    }\n    function parseExpressionAt(input, pos, options) {\n        return Parser.parseExpressionAt(input, pos, options);\n    }\n    function tokenizer(input, options) {\n        return Parser.tokenizer(input, options);\n    }\n    return {\n        version: version,\n        Parser,\n        defaultOptions,\n        Position,\n        SourceLocation,\n        getLineInfo,\n        Node,\n        TokenType,\n        tokTypes,\n        keywordTypes,\n        TokContext,\n        tokContexts,\n        isIdentifierChar,\n        isIdentifierStart,\n        Token,\n        isNewLine,\n        lineBreak,\n        lineBreakG,\n        nonASCIIwhitespace,\n        parse: parse,\n        parseExpressionAt: parseExpressionAt,\n        tokenizer: tokenizer\n    };\n});"]}